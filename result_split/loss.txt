Model: "discriminator"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 32, 32, 32)        2432      
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 32, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 16, 16, 64)        51264     
_________________________________________________________________
batch_normalization_1 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 64)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 16, 16, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 8, 8, 128)         204928    
_________________________________________________________________
batch_normalization_2 (Batch (None, 8, 8, 128)         512       
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 128)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 8, 8, 128)         0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 256)         819456    
_________________________________________________________________
batch_normalization_3 (Batch (None, 4, 4, 256)         1024      
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 4, 4, 256)         0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 2, 2, 512)         3277312   
_________________________________________________________________
batch_normalization_4 (Batch (None, 2, 2, 512)         2048      
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 2, 2, 512)         0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 2, 2, 512)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 2048)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 2049      
_________________________________________________________________
activation_1 (Activation)    (None, 1)                 0         
=================================================================
Total params: 4,361,281
Trainable params: 4,359,361
Non-trainable params: 1,920
_________________________________________________________________
WARNING:tensorflow:From C:\Users\Ultra-QST\PycharmProjects\oldcats\venv\lib\site-packages\tensorflow\python\ops\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Model: "generator"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_2 (Dense)              (None, 32768)             2129920   
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 32768)             0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 8, 8, 512)         0         
_________________________________________________________________
up_sampling2d_1 (UpSampling2 (None, 16, 16, 512)       0         
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, 16, 16, 256)       3277056   
_________________________________________________________________
batch_normalization_5 (Batch (None, 16, 16, 256)       1024      
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 16, 16, 256)       0         
_________________________________________________________________
up_sampling2d_2 (UpSampling2 (None, 32, 32, 256)       0         
_________________________________________________________________
conv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       819328    
_________________________________________________________________
batch_normalization_6 (Batch (None, 32, 32, 128)       512       
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 32, 32, 128)       0         
_________________________________________________________________
up_sampling2d_3 (UpSampling2 (None, 64, 64, 128)       0         
_________________________________________________________________
conv2d_transpose_3 (Conv2DTr (None, 64, 64, 64)        204864    
_________________________________________________________________
batch_normalization_7 (Batch (None, 64, 64, 64)        256       
_________________________________________________________________
leaky_re_lu_9 (LeakyReLU)    (None, 64, 64, 64)        0         
_________________________________________________________________
conv2d_transpose_4 (Conv2DTr (None, 64, 64, 3)         4803      
=================================================================
Total params: 6,437,763
Trainable params: 6,436,867
Non-trainable params: 896
_________________________________________________________________
WARNING:tensorflow:From C:\Users\Ultra-QST\PycharmProjects\oldcats\venv\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

C:\Users\Ultra-QST\PycharmProjects\oldcats\venv\lib\site-packages\keras\engine\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'
C:\Users\Ultra-QST\PycharmProjects\oldcats\venv\lib\site-packages\keras\engine\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'
0 [D loss: 7.731425, acc.: 8.50%] [G loss: 0.001049]
C:\Users\Ultra-QST\PycharmProjects\oldcats\venv\lib\site-packages\keras\engine\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'
100 [D loss: 0.203031, acc.: 98.50%] [G loss: 4.150208]
200 [D loss: 0.400375, acc.: 87.50%] [G loss: 2.859184]
300 [D loss: 0.091789, acc.: 100.00%] [G loss: 4.718996]
400 [D loss: 0.463672, acc.: 74.00%] [G loss: 2.553753]
500 [D loss: 0.140766, acc.: 100.00%] [G loss: 2.663024]
600 [D loss: 0.336132, acc.: 85.00%] [G loss: 2.619856]
700 [D loss: 0.333377, acc.: 88.00%] [G loss: 3.811056]
800 [D loss: 0.213878, acc.: 97.50%] [G loss: 3.616079]
900 [D loss: 0.391936, acc.: 86.50%] [G loss: 3.878429]
1000 [D loss: 1.211802, acc.: 24.50%] [G loss: 3.924774]
1100 [D loss: 0.958186, acc.: 41.00%] [G loss: 3.308653]
1200 [D loss: 0.145601, acc.: 96.50%] [G loss: 5.435125]
1300 [D loss: 0.121999, acc.: 99.00%] [G loss: 3.985725]
1400 [D loss: 0.176974, acc.: 99.00%] [G loss: 3.775994]
1500 [D loss: 0.740298, acc.: 55.50%] [G loss: 3.703436]
1600 [D loss: 0.296759, acc.: 91.50%] [G loss: 3.682516]
1700 [D loss: 0.264699, acc.: 89.00%] [G loss: 2.701629]
1800 [D loss: 0.333936, acc.: 85.50%] [G loss: 2.391285]
1900 [D loss: 0.309693, acc.: 84.00%] [G loss: 3.504607]
2000 [D loss: 0.142946, acc.: 98.50%] [G loss: 4.284491]
2100 [D loss: 0.128059, acc.: 97.50%] [G loss: 4.782922]
2200 [D loss: 0.633123, acc.: 64.50%] [G loss: 4.862553]
2300 [D loss: 1.175610, acc.: 32.00%] [G loss: 3.912570]
2400 [D loss: 0.303923, acc.: 91.00%] [G loss: 3.853333]
2500 [D loss: 0.575173, acc.: 69.00%] [G loss: 4.132321]
2600 [D loss: 0.548256, acc.: 72.50%] [G loss: 3.304295]
2700 [D loss: 0.237166, acc.: 93.00%] [G loss: 2.587066]
2800 [D loss: 0.589315, acc.: 70.00%] [G loss: 3.301177]
2900 [D loss: 0.148872, acc.: 98.50%] [G loss: 2.370607]
3000 [D loss: 1.033153, acc.: 34.50%] [G loss: 3.891593]
3100 [D loss: 0.147703, acc.: 98.50%] [G loss: 3.686043]
3200 [D loss: 0.357030, acc.: 85.00%] [G loss: 3.387341]
3300 [D loss: 0.505691, acc.: 78.00%] [G loss: 4.102497]
3400 [D loss: 0.471512, acc.: 77.50%] [G loss: 2.529835]
3500 [D loss: 0.245090, acc.: 94.50%] [G loss: 2.230995]
3600 [D loss: 0.168378, acc.: 97.50%] [G loss: 3.104793]
3700 [D loss: 0.552416, acc.: 68.50%] [G loss: 3.182591]
3800 [D loss: 0.386118, acc.: 80.50%] [G loss: 4.243948]
3900 [D loss: 0.441798, acc.: 79.50%] [G loss: 2.696479]
4000 [D loss: 0.269644, acc.: 92.00%] [G loss: 3.028980]
4100 [D loss: 0.635477, acc.: 66.00%] [G loss: 4.016716]
4200 [D loss: 0.342980, acc.: 82.50%] [G loss: 2.110869]
4300 [D loss: 0.115302, acc.: 99.00%] [G loss: 2.862286]
4400 [D loss: 0.396380, acc.: 83.00%] [G loss: 2.829202]
4500 [D loss: 0.591201, acc.: 70.50%] [G loss: 2.871123]
4600 [D loss: 0.296814, acc.: 90.00%] [G loss: 1.558836]
4700 [D loss: 0.194342, acc.: 96.50%] [G loss: 2.532560]
4800 [D loss: 0.342582, acc.: 89.00%] [G loss: 3.132730]
4900 [D loss: 0.169847, acc.: 98.50%] [G loss: 4.065207]
5000 [D loss: 0.166415, acc.: 95.50%] [G loss: 3.004838]
5100 [D loss: 0.476877, acc.: 78.00%] [G loss: 3.453115]
5200 [D loss: 0.123485, acc.: 98.50%] [G loss: 2.866160]
5300 [D loss: 0.269479, acc.: 93.00%] [G loss: 1.895521]
5400 [D loss: 0.237436, acc.: 92.00%] [G loss: 3.836223]
5500 [D loss: 0.202648, acc.: 95.00%] [G loss: 2.327672]
5600 [D loss: 0.278359, acc.: 91.50%] [G loss: 3.360232]
5700 [D loss: 0.210552, acc.: 93.50%] [G loss: 3.068349]
5800 [D loss: 0.319067, acc.: 87.00%] [G loss: 2.795171]
5900 [D loss: 0.610108, acc.: 68.00%] [G loss: 2.016707]
6000 [D loss: 0.106314, acc.: 98.00%] [G loss: 3.375910]
6100 [D loss: 0.106044, acc.: 98.50%] [G loss: 1.534076]
6200 [D loss: 0.222252, acc.: 95.00%] [G loss: 5.620943]
6300 [D loss: 0.063154, acc.: 100.00%] [G loss: 3.969871]
6400 [D loss: 0.335983, acc.: 86.00%] [G loss: 4.188514]
6500 [D loss: 0.080515, acc.: 98.50%] [G loss: 3.985546]
6600 [D loss: 0.409207, acc.: 78.00%] [G loss: 2.260124]
6700 [D loss: 0.493594, acc.: 76.00%] [G loss: 3.777591]
6800 [D loss: 0.585803, acc.: 68.50%] [G loss: 3.058616]
6900 [D loss: 0.491908, acc.: 74.50%] [G loss: 1.959359]
7000 [D loss: 0.222139, acc.: 90.50%] [G loss: 3.864992]
7100 [D loss: 0.454479, acc.: 75.50%] [G loss: 4.776070]
7200 [D loss: 0.224417, acc.: 92.00%] [G loss: 3.488683]
7300 [D loss: 0.248487, acc.: 93.00%] [G loss: 2.600808]
7400 [D loss: 0.049094, acc.: 99.50%] [G loss: 3.227266]
7500 [D loss: 0.282269, acc.: 91.50%] [G loss: 3.042972]
7600 [D loss: 0.085512, acc.: 97.50%] [G loss: 5.106205]
7700 [D loss: 0.149526, acc.: 96.50%] [G loss: 4.780727]
7800 [D loss: 0.330552, acc.: 81.50%] [G loss: 4.780754]
7900 [D loss: 0.227754, acc.: 91.00%] [G loss: 4.358795]
8000 [D loss: 0.060736, acc.: 99.50%] [G loss: 5.222921]
8100 [D loss: 0.099263, acc.: 98.50%] [G loss: 3.557837]
8200 [D loss: 0.548687, acc.: 72.00%] [G loss: 2.870220]
8300 [D loss: 0.071758, acc.: 99.00%] [G loss: 5.548192]
8400 [D loss: 0.888599, acc.: 55.50%] [G loss: 3.304785]
8500 [D loss: 0.127820, acc.: 96.00%] [G loss: 5.857075]
8600 [D loss: 0.053378, acc.: 99.00%] [G loss: 5.692333]
8700 [D loss: 0.334580, acc.: 89.00%] [G loss: 4.858961]
8800 [D loss: 0.650637, acc.: 67.00%] [G loss: 6.853038]
8900 [D loss: 0.498433, acc.: 76.50%] [G loss: 5.032378]
9000 [D loss: 0.289332, acc.: 90.00%] [G loss: 3.769819]
9100 [D loss: 0.017771, acc.: 100.00%] [G loss: 5.466702]
9200 [D loss: 0.018550, acc.: 100.00%] [G loss: 4.268519]
9300 [D loss: 0.177648, acc.: 96.50%] [G loss: 5.476841]
9400 [D loss: 0.141188, acc.: 97.50%] [G loss: 3.493727]
9500 [D loss: 0.220375, acc.: 94.00%] [G loss: 4.695436]
9600 [D loss: 0.038145, acc.: 100.00%] [G loss: 2.856027]
9700 [D loss: 0.438539, acc.: 77.50%] [G loss: 2.754641]
9800 [D loss: 0.112752, acc.: 99.00%] [G loss: 3.719691]
9900 [D loss: 0.054824, acc.: 99.00%] [G loss: 3.102526]
10000 [D loss: 0.051467, acc.: 99.50%] [G loss: 4.664107]
10100 [D loss: 0.533551, acc.: 75.00%] [G loss: 5.688904]
10200 [D loss: 0.072408, acc.: 98.50%] [G loss: 2.984351]
10300 [D loss: 0.075846, acc.: 97.50%] [G loss: 8.350966]
10400 [D loss: 0.053081, acc.: 99.00%] [G loss: 6.034992]
10500 [D loss: 0.363853, acc.: 86.50%] [G loss: 3.563403]
10600 [D loss: 0.112476, acc.: 97.00%] [G loss: 2.311644]
10700 [D loss: 0.727420, acc.: 59.50%] [G loss: 9.036227]
10800 [D loss: 0.156450, acc.: 97.00%] [G loss: 3.966225]
10900 [D loss: 0.051743, acc.: 99.00%] [G loss: 6.579826]
11000 [D loss: 0.425536, acc.: 77.50%] [G loss: 4.765727]
11100 [D loss: 0.340214, acc.: 89.50%] [G loss: 3.617866]
11200 [D loss: 0.215307, acc.: 92.00%] [G loss: 4.128497]
11300 [D loss: 0.007385, acc.: 100.00%] [G loss: 7.510226]
11400 [D loss: 0.264401, acc.: 87.50%] [G loss: 4.656486]
11500 [D loss: 0.100967, acc.: 98.00%] [G loss: 3.527994]
11600 [D loss: 0.169891, acc.: 93.50%] [G loss: 4.144069]
11700 [D loss: 0.102571, acc.: 97.50%] [G loss: 6.499963]
11800 [D loss: 0.043130, acc.: 100.00%] [G loss: 2.228864]
11900 [D loss: 0.281895, acc.: 84.50%] [G loss: 5.751121]
12000 [D loss: 0.024179, acc.: 100.00%] [G loss: 6.619580]
12100 [D loss: 0.008174, acc.: 100.00%] [G loss: 4.484816]
12200 [D loss: 0.044660, acc.: 98.50%] [G loss: 4.962996]
12300 [D loss: 0.203112, acc.: 93.50%] [G loss: 6.454967]
12400 [D loss: 0.008346, acc.: 100.00%] [G loss: 9.040420]
12500 [D loss: 0.126690, acc.: 96.00%] [G loss: 3.294357]
12600 [D loss: 0.111867, acc.: 97.00%] [G loss: 5.271697]
12700 [D loss: 0.114792, acc.: 97.50%] [G loss: 4.762207]
12800 [D loss: 0.060696, acc.: 99.00%] [G loss: 4.899328]
12900 [D loss: 0.262017, acc.: 91.50%] [G loss: 3.733680]
13000 [D loss: 0.042321, acc.: 100.00%] [G loss: 5.432550]
13100 [D loss: 0.624794, acc.: 69.00%] [G loss: 4.649836]
13200 [D loss: 0.034874, acc.: 99.00%] [G loss: 4.095402]
13300 [D loss: 0.049273, acc.: 100.00%] [G loss: 5.912931]
13400 [D loss: 0.055989, acc.: 98.50%] [G loss: 6.046834]
13500 [D loss: 0.173150, acc.: 95.00%] [G loss: 3.695862]
13600 [D loss: 0.252114, acc.: 87.00%] [G loss: 3.575793]
13700 [D loss: 0.065282, acc.: 98.50%] [G loss: 4.650964]
13800 [D loss: 0.606618, acc.: 73.00%] [G loss: 9.567244]
13900 [D loss: 0.030800, acc.: 100.00%] [G loss: 3.519334]
14000 [D loss: 0.234012, acc.: 91.00%] [G loss: 3.290340]
14100 [D loss: 0.088389, acc.: 98.50%] [G loss: 3.909212]
14200 [D loss: 0.025193, acc.: 100.00%] [G loss: 6.577048]
14300 [D loss: 0.001425, acc.: 100.00%] [G loss: 10.231974]
14400 [D loss: 0.018183, acc.: 100.00%] [G loss: 5.248187]
14500 [D loss: 0.026359, acc.: 99.00%] [G loss: 4.634753]
14600 [D loss: 0.065155, acc.: 100.00%] [G loss: 4.591979]
14700 [D loss: 0.013965, acc.: 100.00%] [G loss: 5.210860]
14800 [D loss: 0.015472, acc.: 99.50%] [G loss: 3.607358]
14900 [D loss: 0.202491, acc.: 91.50%] [G loss: 4.327919]
15000 [D loss: 0.020928, acc.: 99.50%] [G loss: 6.152014]
15100 [D loss: 0.105376, acc.: 98.50%] [G loss: 5.807365]
15200 [D loss: 0.027438, acc.: 100.00%] [G loss: 2.872235]
15300 [D loss: 0.135674, acc.: 95.50%] [G loss: 7.036196]
15400 [D loss: 0.104594, acc.: 98.00%] [G loss: 4.031736]
15500 [D loss: 0.464212, acc.: 76.50%] [G loss: 9.409298]
15600 [D loss: 0.050420, acc.: 98.50%] [G loss: 6.860568]
15700 [D loss: 0.066792, acc.: 98.00%] [G loss: 4.139452]
15800 [D loss: 0.030855, acc.: 99.00%] [G loss: 6.104080]
15900 [D loss: 0.269319, acc.: 92.00%] [G loss: 5.887118]
16000 [D loss: 0.078291, acc.: 97.50%] [G loss: 7.308400]
16100 [D loss: 0.154954, acc.: 95.50%] [G loss: 6.767109]
16200 [D loss: 0.313939, acc.: 87.50%] [G loss: 3.595882]
16300 [D loss: 0.033530, acc.: 100.00%] [G loss: 5.121155]
16400 [D loss: 0.012998, acc.: 100.00%] [G loss: 9.187177]
16500 [D loss: 0.078594, acc.: 97.50%] [G loss: 4.654115]
16600 [D loss: 0.106259, acc.: 98.00%] [G loss: 1.819691]
16700 [D loss: 0.091071, acc.: 98.50%] [G loss: 6.082348]
16800 [D loss: 0.023217, acc.: 99.50%] [G loss: 3.522210]
16900 [D loss: 0.106928, acc.: 97.50%] [G loss: 5.608768]
17000 [D loss: 0.206430, acc.: 92.50%] [G loss: 1.579711]
17100 [D loss: 0.058432, acc.: 98.50%] [G loss: 3.476593]
17200 [D loss: 0.078085, acc.: 98.50%] [G loss: 6.071124]
17300 [D loss: 0.061708, acc.: 99.00%] [G loss: 5.896643]
17400 [D loss: 0.219598, acc.: 92.50%] [G loss: 2.638026]
17500 [D loss: 0.063747, acc.: 98.50%] [G loss: 7.959407]
17600 [D loss: 0.148969, acc.: 96.00%] [G loss: 5.009326]
17700 [D loss: 0.021281, acc.: 100.00%] [G loss: 5.902702]
17800 [D loss: 0.038098, acc.: 99.50%] [G loss: 2.830061]
17900 [D loss: 0.032301, acc.: 99.00%] [G loss: 7.695405]
18000 [D loss: 0.042055, acc.: 99.50%] [G loss: 3.614256]
18100 [D loss: 0.083947, acc.: 97.00%] [G loss: 2.671044]
18200 [D loss: 0.034591, acc.: 99.50%] [G loss: 4.482023]
18300 [D loss: 0.093808, acc.: 97.50%] [G loss: 9.322187]
18400 [D loss: 0.014283, acc.: 99.50%] [G loss: 7.246237]
18500 [D loss: 0.075694, acc.: 99.00%] [G loss: 4.729654]
18600 [D loss: 0.017472, acc.: 100.00%] [G loss: 2.629108]
18700 [D loss: 0.002264, acc.: 100.00%] [G loss: 5.855394]
18800 [D loss: 0.092825, acc.: 96.50%] [G loss: 5.039368]
18900 [D loss: 0.079650, acc.: 98.50%] [G loss: 4.170482]
19000 [D loss: 0.039603, acc.: 99.00%] [G loss: 5.387418]
19100 [D loss: 0.007629, acc.: 100.00%] [G loss: 6.571363]
19200 [D loss: 0.258526, acc.: 92.50%] [G loss: 7.009623]
19300 [D loss: 0.089569, acc.: 98.50%] [G loss: 5.170311]
19400 [D loss: 0.004034, acc.: 100.00%] [G loss: 3.809704]
19500 [D loss: 0.024636, acc.: 99.50%] [G loss: 6.195110]
19600 [D loss: 0.141770, acc.: 95.00%] [G loss: 5.943739]
19700 [D loss: 0.167836, acc.: 96.00%] [G loss: 6.664868]
19800 [D loss: 0.679875, acc.: 73.50%] [G loss: 3.310786]
19900 [D loss: 0.016329, acc.: 100.00%] [G loss: 2.861557]
20000 [D loss: 0.085569, acc.: 97.00%] [G loss: 5.290521]
20100 [D loss: 0.005848, acc.: 100.00%] [G loss: 10.798108]
20200 [D loss: 0.057967, acc.: 97.50%] [G loss: 4.893897]
20300 [D loss: 0.026048, acc.: 99.00%] [G loss: 4.490180]
20400 [D loss: 0.020758, acc.: 99.50%] [G loss: 8.292541]
20500 [D loss: 0.002106, acc.: 100.00%] [G loss: 9.355799]
20600 [D loss: 0.123333, acc.: 95.50%] [G loss: 4.387666]
20700 [D loss: 0.162899, acc.: 95.00%] [G loss: 5.981545]
20800 [D loss: 0.036048, acc.: 99.00%] [G loss: 9.734533]
20900 [D loss: 0.109693, acc.: 97.00%] [G loss: 7.744292]
21000 [D loss: 0.006948, acc.: 100.00%] [G loss: 3.466551]
21100 [D loss: 0.017683, acc.: 99.50%] [G loss: 5.048815]
21200 [D loss: 0.599768, acc.: 72.00%] [G loss: 2.409734]
21300 [D loss: 0.082046, acc.: 97.00%] [G loss: 5.033678]
21400 [D loss: 1.294543, acc.: 51.00%] [G loss: 5.674135]
21500 [D loss: 0.049215, acc.: 99.50%] [G loss: 5.057010]
21600 [D loss: 0.027429, acc.: 100.00%] [G loss: 9.792163]
21700 [D loss: 0.036676, acc.: 99.00%] [G loss: 7.178977]
21800 [D loss: 0.005285, acc.: 100.00%] [G loss: 4.196617]
21900 [D loss: 0.050902, acc.: 99.00%] [G loss: 7.401803]
22000 [D loss: 0.007287, acc.: 100.00%] [G loss: 8.417098]
22100 [D loss: 0.061276, acc.: 98.50%] [G loss: 4.986401]
22200 [D loss: 0.004326, acc.: 100.00%] [G loss: 8.764597]
22300 [D loss: 0.024310, acc.: 99.50%] [G loss: 2.942397]
22400 [D loss: 0.032119, acc.: 100.00%] [G loss: 5.819387]
22500 [D loss: 0.005103, acc.: 100.00%] [G loss: 8.475672]
22600 [D loss: 0.030415, acc.: 100.00%] [G loss: 4.188771]
22700 [D loss: 0.053536, acc.: 98.00%] [G loss: 4.845705]
22800 [D loss: 0.171831, acc.: 94.50%] [G loss: 7.441469]
22900 [D loss: 0.036421, acc.: 99.50%] [G loss: 3.834299]
23000 [D loss: 0.052276, acc.: 98.50%] [G loss: 6.906426]
23100 [D loss: 0.015908, acc.: 100.00%] [G loss: 4.435972]
23200 [D loss: 0.713349, acc.: 61.00%] [G loss: 6.675780]
23300 [D loss: 0.008526, acc.: 100.00%] [G loss: 7.127986]
23400 [D loss: 0.016701, acc.: 100.00%] [G loss: 7.580667]
23500 [D loss: 0.070876, acc.: 98.00%] [G loss: 5.953226]
23600 [D loss: 0.249042, acc.: 93.00%] [G loss: 6.701588]
23700 [D loss: 0.045027, acc.: 100.00%] [G loss: 11.781520]
23800 [D loss: 0.010973, acc.: 100.00%] [G loss: 4.658306]
23900 [D loss: 0.021655, acc.: 99.50%] [G loss: 10.697033]
24000 [D loss: 0.092788, acc.: 97.50%] [G loss: 7.080983]
24100 [D loss: 0.008599, acc.: 99.50%] [G loss: 4.884326]
24200 [D loss: 0.045751, acc.: 98.50%] [G loss: 14.449161]
24300 [D loss: 0.012189, acc.: 100.00%] [G loss: 5.438647]
24400 [D loss: 0.015272, acc.: 99.50%] [G loss: 3.513216]
24500 [D loss: 0.003786, acc.: 100.00%] [G loss: 3.002658]
24600 [D loss: 0.386561, acc.: 79.50%] [G loss: 14.010180]
24700 [D loss: 0.030518, acc.: 99.00%] [G loss: 8.076478]
24800 [D loss: 0.005121, acc.: 100.00%] [G loss: 5.372500]
24900 [D loss: 0.028862, acc.: 100.00%] [G loss: 6.822764]
25000 [D loss: 0.001572, acc.: 100.00%] [G loss: 10.199961]
25100 [D loss: 0.023729, acc.: 99.50%] [G loss: 5.070008]
25200 [D loss: 0.118578, acc.: 96.50%] [G loss: 7.291845]
25300 [D loss: 0.001887, acc.: 100.00%] [G loss: 13.685905]
25400 [D loss: 0.024148, acc.: 99.50%] [G loss: 8.255404]
25500 [D loss: 0.306369, acc.: 85.00%] [G loss: 6.069081]
25600 [D loss: 0.098772, acc.: 97.50%] [G loss: 7.248901]
25700 [D loss: 0.007864, acc.: 100.00%] [G loss: 2.511437]
25800 [D loss: 0.001927, acc.: 100.00%] [G loss: 2.749021]
25900 [D loss: 0.096200, acc.: 97.50%] [G loss: 2.506672]
26000 [D loss: 0.011245, acc.: 100.00%] [G loss: 6.719224]
26100 [D loss: 0.004328, acc.: 100.00%] [G loss: 7.588741]
26200 [D loss: 0.039745, acc.: 100.00%] [G loss: 9.160827]
26300 [D loss: 0.004049, acc.: 100.00%] [G loss: 3.737278]
26400 [D loss: 0.028980, acc.: 99.50%] [G loss: 5.704117]
26500 [D loss: 0.002108, acc.: 100.00%] [G loss: 12.514680]
26600 [D loss: 0.076947, acc.: 98.50%] [G loss: 7.957278]
26700 [D loss: 0.014322, acc.: 100.00%] [G loss: 8.740221]
26800 [D loss: 0.144167, acc.: 95.00%] [G loss: 3.424598]
26900 [D loss: 0.000777, acc.: 100.00%] [G loss: 7.741519]
27000 [D loss: 0.100992, acc.: 97.00%] [G loss: 9.402769]
27100 [D loss: 0.146458, acc.: 95.00%] [G loss: 5.716014]
27200 [D loss: 0.469025, acc.: 77.50%] [G loss: 3.388553]
27300 [D loss: 0.009830, acc.: 100.00%] [G loss: 7.253872]
27400 [D loss: 0.019980, acc.: 99.50%] [G loss: 7.910633]
27500 [D loss: 0.052687, acc.: 99.00%] [G loss: 4.995547]
27600 [D loss: 0.002309, acc.: 100.00%] [G loss: 9.076498]
27700 [D loss: 0.031732, acc.: 98.50%] [G loss: 9.786053]
27800 [D loss: 0.214718, acc.: 89.50%] [G loss: 9.434524]
27900 [D loss: 0.178690, acc.: 93.00%] [G loss: 15.402070]
28000 [D loss: 0.001698, acc.: 100.00%] [G loss: 7.644490]
28100 [D loss: 0.010796, acc.: 100.00%] [G loss: 7.958129]
28200 [D loss: 0.000884, acc.: 100.00%] [G loss: 8.494558]
28300 [D loss: 0.001006, acc.: 100.00%] [G loss: 7.777905]
28400 [D loss: 0.042441, acc.: 99.50%] [G loss: 7.834137]
28500 [D loss: 0.278236, acc.: 92.50%] [G loss: 9.204777]
28600 [D loss: 0.011252, acc.: 99.50%] [G loss: 4.876799]
28700 [D loss: 0.002371, acc.: 100.00%] [G loss: 7.157109]
28800 [D loss: 0.401724, acc.: 79.50%] [G loss: 5.470315]
28900 [D loss: 0.018495, acc.: 99.50%] [G loss: 8.197984]
29000 [D loss: 0.008636, acc.: 100.00%] [G loss: 5.554356]
29100 [D loss: 0.172384, acc.: 94.00%] [G loss: 9.751605]
29200 [D loss: 0.011723, acc.: 100.00%] [G loss: 8.095369]
29300 [D loss: 0.032579, acc.: 100.00%] [G loss: 5.825185]
29400 [D loss: 0.093620, acc.: 97.00%] [G loss: 6.149724]
29500 [D loss: 0.009648, acc.: 99.50%] [G loss: 4.977060]
29600 [D loss: 0.035590, acc.: 99.50%] [G loss: 8.057640]
29700 [D loss: 0.064914, acc.: 99.00%] [G loss: 4.724108]
29800 [D loss: 0.007779, acc.: 100.00%] [G loss: 5.423207]
29900 [D loss: 0.007150, acc.: 100.00%] [G loss: 7.257946]
30000 [D loss: 0.007752, acc.: 100.00%] [G loss: 8.023890]

Process finished with exit code 0
