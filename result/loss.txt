Model: "discriminator"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 32, 32, 32)        2432      
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 32, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 16, 16, 64)        51264     
_________________________________________________________________
batch_normalization_1 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 64)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 16, 16, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 8, 8, 128)         204928    
_________________________________________________________________
batch_normalization_2 (Batch (None, 8, 8, 128)         512       
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 128)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 8, 8, 128)         0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 256)         819456    
_________________________________________________________________
batch_normalization_3 (Batch (None, 4, 4, 256)         1024      
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 4, 4, 256)         0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 2, 2, 512)         3277312   
_________________________________________________________________
batch_normalization_4 (Batch (None, 2, 2, 512)         2048      
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 2, 2, 512)         0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 2, 2, 512)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 2048)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 2049      
_________________________________________________________________
activation_1 (Activation)    (None, 1)                 0         
=================================================================
Total params: 4,361,281
Trainable params: 4,359,361
Non-trainable params: 1,920
_________________________________________________________________
WARNING:tensorflow:From C:\Users\Ultra-QST\PycharmProjects\oldcats\venv\lib\site-packages\tensorflow\python\ops\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Model: "generator"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_2 (Dense)              (None, 32768)             2129920   
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 32768)             0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 8, 8, 512)         0         
_________________________________________________________________
up_sampling2d_1 (UpSampling2 (None, 16, 16, 512)       0         
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, 16, 16, 256)       3277056   
_________________________________________________________________
batch_normalization_5 (Batch (None, 16, 16, 256)       1024      
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 16, 16, 256)       0         
_________________________________________________________________
up_sampling2d_2 (UpSampling2 (None, 32, 32, 256)       0         
_________________________________________________________________
conv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       819328    
_________________________________________________________________
batch_normalization_6 (Batch (None, 32, 32, 128)       512       
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 32, 32, 128)       0         
_________________________________________________________________
up_sampling2d_3 (UpSampling2 (None, 64, 64, 128)       0         
_________________________________________________________________
conv2d_transpose_3 (Conv2DTr (None, 64, 64, 64)        204864    
_________________________________________________________________
batch_normalization_7 (Batch (None, 64, 64, 64)        256       
_________________________________________________________________
leaky_re_lu_9 (LeakyReLU)    (None, 64, 64, 64)        0         
_________________________________________________________________
conv2d_transpose_4 (Conv2DTr (None, 64, 64, 3)         4803      
=================================================================
Total params: 6,437,763
Trainable params: 6,436,867
Non-trainable params: 896
_________________________________________________________________
WARNING:tensorflow:From C:\Users\Ultra-QST\PycharmProjects\oldcats\venv\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

C:\Users\Ultra-QST\PycharmProjects\oldcats\venv\lib\site-packages\keras\engine\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'
C:\Users\Ultra-QST\PycharmProjects\oldcats\venv\lib\site-packages\keras\engine\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'
0 [D loss: 7.996758, acc.: 28.00%] [G loss: 0.001189]
C:\Users\Ultra-QST\PycharmProjects\oldcats\venv\lib\site-packages\keras\engine\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
  'Discrepancy between trainable weights and collected trainable'
100 [D loss: 0.247875, acc.: 93.50%] [G loss: 1.349581]
200 [D loss: 0.497615, acc.: 80.50%] [G loss: 1.511131]
300 [D loss: 0.385419, acc.: 89.00%] [G loss: 0.589790]
400 [D loss: 1.031864, acc.: 51.00%] [G loss: 2.705090]
500 [D loss: 0.355059, acc.: 81.50%] [G loss: 1.809795]
600 [D loss: 0.336980, acc.: 84.50%] [G loss: 3.143427]
700 [D loss: 0.642907, acc.: 62.00%] [G loss: 3.308684]
800 [D loss: 0.153369, acc.: 99.50%] [G loss: 2.869092]
900 [D loss: 0.625285, acc.: 58.00%] [G loss: 2.267286]
1000 [D loss: 0.582573, acc.: 60.50%] [G loss: 3.704834]
1100 [D loss: 0.450476, acc.: 75.00%] [G loss: 2.598485]
1200 [D loss: 0.761739, acc.: 56.50%] [G loss: 3.808401]
1300 [D loss: 1.434739, acc.: 6.00%] [G loss: 4.718869]
1400 [D loss: 0.514139, acc.: 75.50%] [G loss: 1.253207]
1500 [D loss: 0.297379, acc.: 95.50%] [G loss: 2.832741]
1600 [D loss: 0.836916, acc.: 53.50%] [G loss: 6.883610]
1700 [D loss: 0.271296, acc.: 91.50%] [G loss: 3.799419]
1800 [D loss: 0.440339, acc.: 79.50%] [G loss: 3.219927]
1900 [D loss: 0.137213, acc.: 98.50%] [G loss: 3.777375]
2000 [D loss: 0.278661, acc.: 96.00%] [G loss: 3.658675]
2100 [D loss: 0.661309, acc.: 62.00%] [G loss: 2.608212]
2200 [D loss: 0.207490, acc.: 96.50%] [G loss: 3.406011]
2300 [D loss: 0.678582, acc.: 60.00%] [G loss: 4.404996]
2400 [D loss: 0.269833, acc.: 94.50%] [G loss: 2.254105]
2500 [D loss: 0.115724, acc.: 99.00%] [G loss: 3.976799]
2600 [D loss: 0.107916, acc.: 99.00%] [G loss: 2.380933]
2700 [D loss: 0.133699, acc.: 98.00%] [G loss: 3.614827]
2800 [D loss: 0.256543, acc.: 95.00%] [G loss: 3.501640]
2900 [D loss: 0.252433, acc.: 94.50%] [G loss: 4.719686]
3000 [D loss: 0.360664, acc.: 86.00%] [G loss: 2.879502]
3100 [D loss: 1.247114, acc.: 24.50%] [G loss: 4.549340]
3200 [D loss: 0.352975, acc.: 84.50%] [G loss: 1.960028]
3300 [D loss: 1.316593, acc.: 43.50%] [G loss: 5.360088]
3400 [D loss: 0.346958, acc.: 86.00%] [G loss: 2.695206]
3500 [D loss: 0.489240, acc.: 75.00%] [G loss: 2.729447]
3600 [D loss: 0.351692, acc.: 86.50%] [G loss: 2.960591]
3700 [D loss: 0.143935, acc.: 99.00%] [G loss: 3.276347]
3800 [D loss: 0.457704, acc.: 80.00%] [G loss: 2.239470]
3900 [D loss: 0.965437, acc.: 41.50%] [G loss: 2.320046]
4000 [D loss: 0.229857, acc.: 93.00%] [G loss: 2.961470]
4100 [D loss: 1.363281, acc.: 21.50%] [G loss: 3.837584]
4200 [D loss: 0.509220, acc.: 73.00%] [G loss: 5.851449]
4300 [D loss: 0.648006, acc.: 69.00%] [G loss: 2.308917]
4400 [D loss: 0.221230, acc.: 97.00%] [G loss: 3.036864]
4500 [D loss: 0.225584, acc.: 92.50%] [G loss: 2.232128]
4600 [D loss: 0.184561, acc.: 97.00%] [G loss: 2.592844]
4700 [D loss: 0.319063, acc.: 89.00%] [G loss: 1.782030]
4800 [D loss: 0.235754, acc.: 95.00%] [G loss: 4.728227]
4900 [D loss: 0.431258, acc.: 80.00%] [G loss: 5.152255]
5000 [D loss: 0.289666, acc.: 89.50%] [G loss: 2.635886]
5100 [D loss: 0.177728, acc.: 97.00%] [G loss: 3.207670]
5200 [D loss: 0.262828, acc.: 92.50%] [G loss: 2.422396]
5300 [D loss: 0.230996, acc.: 92.00%] [G loss: 3.632538]
5400 [D loss: 0.232601, acc.: 89.50%] [G loss: 3.403232]
5500 [D loss: 0.116559, acc.: 99.00%] [G loss: 2.535473]
5600 [D loss: 0.392670, acc.: 85.50%] [G loss: 3.230980]
5700 [D loss: 1.150435, acc.: 30.00%] [G loss: 3.657294]
5800 [D loss: 0.417906, acc.: 83.00%] [G loss: 4.299860]
5900 [D loss: 0.318349, acc.: 88.50%] [G loss: 4.581205]
6000 [D loss: 0.700393, acc.: 64.50%] [G loss: 2.692879]
6100 [D loss: 0.227734, acc.: 96.00%] [G loss: 3.201934]
6200 [D loss: 0.198058, acc.: 96.00%] [G loss: 2.608411]
6300 [D loss: 0.228469, acc.: 92.00%] [G loss: 3.553338]
6400 [D loss: 0.670755, acc.: 64.50%] [G loss: 2.736589]
6500 [D loss: 0.235650, acc.: 88.50%] [G loss: 4.925521]
6600 [D loss: 0.287111, acc.: 88.00%] [G loss: 4.216878]
6700 [D loss: 0.182063, acc.: 94.50%] [G loss: 2.006879]
6800 [D loss: 0.101516, acc.: 98.00%] [G loss: 5.645306]
6900 [D loss: 0.139504, acc.: 98.00%] [G loss: 1.659648]
7000 [D loss: 0.037194, acc.: 100.00%] [G loss: 2.879745]
7100 [D loss: 0.483258, acc.: 76.50%] [G loss: 2.041479]
7200 [D loss: 0.558776, acc.: 70.00%] [G loss: 6.490304]
7300 [D loss: 0.388331, acc.: 81.00%] [G loss: 3.612431]
7400 [D loss: 0.353292, acc.: 86.50%] [G loss: 2.516954]
7500 [D loss: 0.060139, acc.: 99.50%] [G loss: 7.586000]
7600 [D loss: 0.896361, acc.: 55.50%] [G loss: 5.747366]
7700 [D loss: 0.134206, acc.: 97.50%] [G loss: 3.266155]
7800 [D loss: 0.178366, acc.: 95.50%] [G loss: 3.920838]
7900 [D loss: 0.099814, acc.: 98.50%] [G loss: 4.087271]
8000 [D loss: 0.105850, acc.: 98.00%] [G loss: 4.842833]
8100 [D loss: 0.086124, acc.: 98.00%] [G loss: 1.568262]
8200 [D loss: 0.205279, acc.: 96.00%] [G loss: 1.805951]
8300 [D loss: 1.137531, acc.: 38.50%] [G loss: 6.774805]
8400 [D loss: 0.156121, acc.: 96.00%] [G loss: 3.297142]
8500 [D loss: 0.093195, acc.: 99.00%] [G loss: 4.152284]
8600 [D loss: 0.161375, acc.: 97.50%] [G loss: 4.229943]
8700 [D loss: 0.105721, acc.: 98.50%] [G loss: 5.171302]
8800 [D loss: 0.892467, acc.: 60.50%] [G loss: 5.204372]
8900 [D loss: 0.036697, acc.: 100.00%] [G loss: 3.371089]
9000 [D loss: 0.377054, acc.: 78.50%] [G loss: 5.209988]
9100 [D loss: 0.107478, acc.: 96.50%] [G loss: 6.041773]
9200 [D loss: 0.122917, acc.: 97.50%] [G loss: 2.725831]
9300 [D loss: 0.144896, acc.: 95.50%] [G loss: 4.652055]
9400 [D loss: 0.037897, acc.: 100.00%] [G loss: 2.759015]
9500 [D loss: 0.096876, acc.: 98.50%] [G loss: 3.230833]
9600 [D loss: 0.269311, acc.: 89.00%] [G loss: 5.229938]
9700 [D loss: 0.177926, acc.: 96.00%] [G loss: 3.951411]
9800 [D loss: 0.103330, acc.: 97.00%] [G loss: 3.165267]
9900 [D loss: 0.065018, acc.: 100.00%] [G loss: 3.573597]
10000 [D loss: 0.293477, acc.: 91.50%] [G loss: 3.821295]
10100 [D loss: 2.083805, acc.: 19.00%] [G loss: 7.165198]
10200 [D loss: 0.037395, acc.: 99.00%] [G loss: 6.216372]
10300 [D loss: 0.342513, acc.: 84.50%] [G loss: 0.944814]
10400 [D loss: 0.161682, acc.: 94.50%] [G loss: 4.304504]
10500 [D loss: 0.111688, acc.: 96.50%] [G loss: 4.490817]
10600 [D loss: 0.077155, acc.: 99.50%] [G loss: 9.571693]
10700 [D loss: 0.107941, acc.: 96.50%] [G loss: 2.981407]
10800 [D loss: 0.537822, acc.: 73.50%] [G loss: 8.624119]
10900 [D loss: 0.077281, acc.: 99.50%] [G loss: 3.991816]
11000 [D loss: 0.062787, acc.: 99.00%] [G loss: 3.759683]
11100 [D loss: 0.218365, acc.: 91.00%] [G loss: 4.345541]
11200 [D loss: 0.130651, acc.: 97.50%] [G loss: 4.396336]
11300 [D loss: 0.349045, acc.: 84.00%] [G loss: 3.060462]
11400 [D loss: 0.067656, acc.: 99.50%] [G loss: 1.508023]
11500 [D loss: 0.042846, acc.: 99.50%] [G loss: 3.617124]
11600 [D loss: 0.163593, acc.: 94.00%] [G loss: 4.656670]
11700 [D loss: 0.346990, acc.: 89.50%] [G loss: 0.764661]
11800 [D loss: 0.014035, acc.: 100.00%] [G loss: 7.534903]
11900 [D loss: 0.197805, acc.: 92.00%] [G loss: 5.622958]
12000 [D loss: 0.079162, acc.: 99.00%] [G loss: 5.754922]
12100 [D loss: 0.026977, acc.: 100.00%] [G loss: 6.613586]
12200 [D loss: 0.014670, acc.: 100.00%] [G loss: 11.617570]
12300 [D loss: 0.161380, acc.: 96.50%] [G loss: 6.886080]
12400 [D loss: 0.113413, acc.: 98.00%] [G loss: 3.720040]
12500 [D loss: 0.018352, acc.: 100.00%] [G loss: 4.680555]
12600 [D loss: 0.056279, acc.: 100.00%] [G loss: 3.152975]
12700 [D loss: 0.363172, acc.: 81.50%] [G loss: 9.476414]
12800 [D loss: 0.006670, acc.: 100.00%] [G loss: 4.534328]
12900 [D loss: 0.112995, acc.: 96.00%] [G loss: 6.637706]
13000 [D loss: 0.059876, acc.: 99.00%] [G loss: 2.861592]
13100 [D loss: 0.010143, acc.: 100.00%] [G loss: 2.689594]
13200 [D loss: 0.023307, acc.: 100.00%] [G loss: 5.883273]
13300 [D loss: 0.027149, acc.: 99.50%] [G loss: 3.982213]
13400 [D loss: 0.018818, acc.: 100.00%] [G loss: 3.755665]
13500 [D loss: 0.064675, acc.: 98.00%] [G loss: 6.242682]
13600 [D loss: 0.011085, acc.: 99.50%] [G loss: 6.461998]
13700 [D loss: 0.194836, acc.: 95.00%] [G loss: 7.294652]
13800 [D loss: 0.046565, acc.: 99.00%] [G loss: 6.381248]
13900 [D loss: 0.128479, acc.: 98.00%] [G loss: 3.411616]
14000 [D loss: 0.068649, acc.: 98.50%] [G loss: 3.939528]
14100 [D loss: 0.032942, acc.: 99.00%] [G loss: 7.306213]
14200 [D loss: 0.058222, acc.: 98.00%] [G loss: 1.736914]
14300 [D loss: 0.015991, acc.: 99.50%] [G loss: 5.472325]
14400 [D loss: 0.021720, acc.: 100.00%] [G loss: 7.048320]
14500 [D loss: 0.334227, acc.: 81.00%] [G loss: 2.226274]
14600 [D loss: 0.214077, acc.: 91.00%] [G loss: 2.148970]
14700 [D loss: 0.022708, acc.: 100.00%] [G loss: 5.931691]
14800 [D loss: 0.010274, acc.: 99.50%] [G loss: 9.317021]
14900 [D loss: 0.107097, acc.: 98.00%] [G loss: 5.799362]
15000 [D loss: 0.008349, acc.: 100.00%] [G loss: 2.233490]
15100 [D loss: 0.124370, acc.: 95.50%] [G loss: 6.246576]
15200 [D loss: 0.038401, acc.: 99.50%] [G loss: 3.458629]
15300 [D loss: 0.139699, acc.: 96.00%] [G loss: 3.428487]
15400 [D loss: 0.061591, acc.: 99.00%] [G loss: 5.680773]
15500 [D loss: 0.597636, acc.: 68.00%] [G loss: 3.980737]
15600 [D loss: 1.086910, acc.: 56.50%] [G loss: 6.845640]
15700 [D loss: 0.159274, acc.: 95.00%] [G loss: 5.540285]
15800 [D loss: 0.023781, acc.: 100.00%] [G loss: 11.218204]
15900 [D loss: 0.003117, acc.: 100.00%] [G loss: 4.747459]
16000 [D loss: 0.015283, acc.: 100.00%] [G loss: 4.347957]
16100 [D loss: 0.227576, acc.: 92.50%] [G loss: 9.250183]
16200 [D loss: 0.044758, acc.: 100.00%] [G loss: 9.719793]
16300 [D loss: 0.064824, acc.: 100.00%] [G loss: 6.302938]
16400 [D loss: 0.568597, acc.: 74.00%] [G loss: 1.403664]
16500 [D loss: 0.127547, acc.: 97.50%] [G loss: 3.746064]
16600 [D loss: 0.004290, acc.: 100.00%] [G loss: 8.557541]
16700 [D loss: 0.295440, acc.: 83.00%] [G loss: 6.865774]
16800 [D loss: 0.002128, acc.: 100.00%] [G loss: 6.430888]
16900 [D loss: 0.180045, acc.: 94.00%] [G loss: 5.966483]
17000 [D loss: 0.051891, acc.: 99.00%] [G loss: 8.101779]
17100 [D loss: 0.477793, acc.: 73.50%] [G loss: 0.567270]
17200 [D loss: 0.082569, acc.: 97.50%] [G loss: 5.380667]
17300 [D loss: 0.133059, acc.: 97.50%] [G loss: 3.913920]
17400 [D loss: 0.051159, acc.: 98.50%] [G loss: 5.813575]
17500 [D loss: 0.013909, acc.: 100.00%] [G loss: 5.172472]
17600 [D loss: 0.031555, acc.: 99.50%] [G loss: 9.765604]
17700 [D loss: 0.011520, acc.: 100.00%] [G loss: 13.074904]
17800 [D loss: 0.231985, acc.: 88.50%] [G loss: 4.460145]
17900 [D loss: 0.109604, acc.: 96.50%] [G loss: 7.096030]
18000 [D loss: 0.329836, acc.: 84.50%] [G loss: 4.926932]
18100 [D loss: 0.023544, acc.: 99.50%] [G loss: 3.038663]
18200 [D loss: 0.136051, acc.: 94.50%] [G loss: 4.584089]
18300 [D loss: 0.059206, acc.: 99.00%] [G loss: 11.691947]
18400 [D loss: 0.039300, acc.: 99.50%] [G loss: 4.628714]
18500 [D loss: 0.142066, acc.: 94.00%] [G loss: 5.979710]
18600 [D loss: 1.755896, acc.: 51.50%] [G loss: 3.033159]
18700 [D loss: 0.035982, acc.: 99.50%] [G loss: 4.305967]
18800 [D loss: 0.093074, acc.: 98.00%] [G loss: 5.443008]
18900 [D loss: 0.033430, acc.: 100.00%] [G loss: 3.779477]
19000 [D loss: 0.023634, acc.: 100.00%] [G loss: 2.988271]
19100 [D loss: 0.006931, acc.: 100.00%] [G loss: 8.307899]
19200 [D loss: 0.021744, acc.: 99.50%] [G loss: 4.815481]
19300 [D loss: 0.003932, acc.: 100.00%] [G loss: 10.002636]
19400 [D loss: 0.060232, acc.: 98.00%] [G loss: 5.190919]
19500 [D loss: 0.120322, acc.: 97.50%] [G loss: 6.744723]
19600 [D loss: 0.012365, acc.: 99.50%] [G loss: 5.935216]
19700 [D loss: 0.030191, acc.: 99.50%] [G loss: 6.221795]
19800 [D loss: 0.300419, acc.: 85.50%] [G loss: 2.787377]
19900 [D loss: 0.002168, acc.: 100.00%] [G loss: 9.930507]
20000 [D loss: 0.034116, acc.: 99.50%] [G loss: 7.990255]
20100 [D loss: 0.043297, acc.: 99.00%] [G loss: 2.714332]
20200 [D loss: 0.216515, acc.: 92.00%] [G loss: 4.400388]
20300 [D loss: 0.006212, acc.: 100.00%] [G loss: 6.700093]
20400 [D loss: 0.244589, acc.: 92.50%] [G loss: 5.733211]
20500 [D loss: 1.011011, acc.: 62.00%] [G loss: 5.114124]
20600 [D loss: 0.017881, acc.: 99.50%] [G loss: 6.428658]
20700 [D loss: 0.115599, acc.: 96.00%] [G loss: 5.411375]
20800 [D loss: 0.011967, acc.: 100.00%] [G loss: 9.029193]
20900 [D loss: 0.019898, acc.: 100.00%] [G loss: 2.586554]
21000 [D loss: 0.007927, acc.: 100.00%] [G loss: 6.873189]
21100 [D loss: 0.098579, acc.: 97.00%] [G loss: 7.918612]
21200 [D loss: 0.357918, acc.: 86.00%] [G loss: 10.597478]
21300 [D loss: 0.123419, acc.: 96.00%] [G loss: 6.609645]
21400 [D loss: 0.062365, acc.: 98.50%] [G loss: 14.225153]
21500 [D loss: 0.083075, acc.: 98.50%] [G loss: 7.599411]
21600 [D loss: 0.018964, acc.: 99.50%] [G loss: 3.770620]
21700 [D loss: 0.029345, acc.: 99.50%] [G loss: 2.721903]
21800 [D loss: 0.025739, acc.: 99.50%] [G loss: 6.298402]
21900 [D loss: 0.028925, acc.: 99.50%] [G loss: 6.731899]
22000 [D loss: 0.832775, acc.: 62.50%] [G loss: 5.158539]
22100 [D loss: 0.003362, acc.: 100.00%] [G loss: 6.315539]
22200 [D loss: 0.018205, acc.: 100.00%] [G loss: 6.945627]
22300 [D loss: 0.005267, acc.: 100.00%] [G loss: 4.150246]
22400 [D loss: 0.024256, acc.: 100.00%] [G loss: 2.919123]
22500 [D loss: 0.000319, acc.: 100.00%] [G loss: 6.873853]
22600 [D loss: 0.013701, acc.: 100.00%] [G loss: 7.305301]
22700 [D loss: 0.706344, acc.: 67.00%] [G loss: 8.238889]
22800 [D loss: 0.333210, acc.: 85.00%] [G loss: 9.443546]
22900 [D loss: 0.039953, acc.: 99.00%] [G loss: 4.365413]
23000 [D loss: 0.000533, acc.: 100.00%] [G loss: 14.239552]
23100 [D loss: 0.062738, acc.: 98.00%] [G loss: 5.265765]
23200 [D loss: 0.121954, acc.: 97.00%] [G loss: 2.612813]
23300 [D loss: 0.253026, acc.: 85.00%] [G loss: 8.742190]
23400 [D loss: 0.016217, acc.: 100.00%] [G loss: 6.020828]
23500 [D loss: 0.001387, acc.: 100.00%] [G loss: 6.981759]
23600 [D loss: 0.010880, acc.: 100.00%] [G loss: 3.769001]
23700 [D loss: 0.033249, acc.: 99.50%] [G loss: 8.301902]
23800 [D loss: 0.014155, acc.: 100.00%] [G loss: 4.922301]
23900 [D loss: 0.145061, acc.: 97.00%] [G loss: 5.756725]
24000 [D loss: 0.000268, acc.: 100.00%] [G loss: 11.833364]
24100 [D loss: 1.847028, acc.: 22.00%] [G loss: 10.288507]
24200 [D loss: 0.014334, acc.: 100.00%] [G loss: 6.708629]
24300 [D loss: 0.003207, acc.: 100.00%] [G loss: 8.051449]
24400 [D loss: 0.039910, acc.: 99.00%] [G loss: 7.838613]
24500 [D loss: 0.038838, acc.: 99.00%] [G loss: 4.468556]
24600 [D loss: 0.012063, acc.: 100.00%] [G loss: 9.960383]
24700 [D loss: 0.179716, acc.: 95.00%] [G loss: 9.015290]
24800 [D loss: 0.084394, acc.: 98.50%] [G loss: 8.652366]
24900 [D loss: 0.018834, acc.: 100.00%] [G loss: 2.511657]
25000 [D loss: 0.003759, acc.: 100.00%] [G loss: 4.511154]
25100 [D loss: 0.000799, acc.: 100.00%] [G loss: 19.420336]
25200 [D loss: 0.002450, acc.: 100.00%] [G loss: 6.504428]
25300 [D loss: 0.061420, acc.: 98.50%] [G loss: 6.703334]
25400 [D loss: 0.067775, acc.: 98.50%] [G loss: 6.702183]
25500 [D loss: 0.024204, acc.: 99.50%] [G loss: 5.128213]
25600 [D loss: 0.011867, acc.: 100.00%] [G loss: 11.064664]
25700 [D loss: 1.370945, acc.: 47.00%] [G loss: 0.738902]
25800 [D loss: 0.045209, acc.: 99.50%] [G loss: 6.009365]
25900 [D loss: 0.019768, acc.: 100.00%] [G loss: 4.399434]
26000 [D loss: 0.782068, acc.: 61.50%] [G loss: 3.755105]
26100 [D loss: 0.021131, acc.: 100.00%] [G loss: 9.312604]
26200 [D loss: 0.004161, acc.: 100.00%] [G loss: 4.954158]
26300 [D loss: 0.052832, acc.: 99.50%] [G loss: 7.207732]
26400 [D loss: 0.027343, acc.: 100.00%] [G loss: 7.336834]
26500 [D loss: 0.010465, acc.: 100.00%] [G loss: 3.467481]
26600 [D loss: 0.166388, acc.: 95.00%] [G loss: 7.109472]
26700 [D loss: 0.005736, acc.: 100.00%] [G loss: 1.864395]
26800 [D loss: 0.075988, acc.: 98.50%] [G loss: 7.493628]
26900 [D loss: 0.037440, acc.: 98.50%] [G loss: 5.123338]
27000 [D loss: 0.005724, acc.: 100.00%] [G loss: 10.824873]
27100 [D loss: 0.034116, acc.: 100.00%] [G loss: 11.576244]
27200 [D loss: 0.252226, acc.: 94.00%] [G loss: 8.611683]
27300 [D loss: 0.002997, acc.: 100.00%] [G loss: 5.725431]
27400 [D loss: 0.152476, acc.: 95.00%] [G loss: 7.322342]
27500 [D loss: 0.082043, acc.: 97.00%] [G loss: 7.170824]
27600 [D loss: 0.016540, acc.: 100.00%] [G loss: 5.861610]
27700 [D loss: 0.094649, acc.: 97.00%] [G loss: 6.962091]
27800 [D loss: 0.020757, acc.: 99.50%] [G loss: 6.816338]
27900 [D loss: 0.057127, acc.: 98.50%] [G loss: 6.853789]
28000 [D loss: 0.019122, acc.: 100.00%] [G loss: 5.846860]
28100 [D loss: 0.044350, acc.: 98.50%] [G loss: 4.997993]
28200 [D loss: 0.010874, acc.: 100.00%] [G loss: 7.998179]
28300 [D loss: 0.021328, acc.: 100.00%] [G loss: 3.568526]
28400 [D loss: 0.410141, acc.: 78.50%] [G loss: 11.364353]
28500 [D loss: 0.000527, acc.: 100.00%] [G loss: 15.108474]
28600 [D loss: 0.027449, acc.: 99.50%] [G loss: 4.145150]
28700 [D loss: 0.007835, acc.: 99.50%] [G loss: 7.777535]
28800 [D loss: 0.053020, acc.: 99.00%] [G loss: 6.659082]
28900 [D loss: 0.001908, acc.: 100.00%] [G loss: 4.552996]
29000 [D loss: 0.001121, acc.: 100.00%] [G loss: 10.865378]
29100 [D loss: 0.001297, acc.: 100.00%] [G loss: 5.658010]
29200 [D loss: 0.129244, acc.: 97.00%] [G loss: 5.171171]
29300 [D loss: 0.005961, acc.: 100.00%] [G loss: 4.359957]
29400 [D loss: 0.125258, acc.: 95.00%] [G loss: 10.858604]
29500 [D loss: 0.115750, acc.: 96.00%] [G loss: 9.205894]
29600 [D loss: 0.018906, acc.: 99.50%] [G loss: 5.305424]
29700 [D loss: 0.087961, acc.: 98.00%] [G loss: 5.342026]
29800 [D loss: 0.070440, acc.: 98.50%] [G loss: 5.516135]
29900 [D loss: 0.014501, acc.: 99.50%] [G loss: 8.702941]
30000 [D loss: 0.001598, acc.: 100.00%] [G loss: 9.922344]

Process finished with exit code 0
